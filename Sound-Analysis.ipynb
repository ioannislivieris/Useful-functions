{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "#\n",
    "import pandas as pd\n",
    "import os\n",
    "from   tqdm    import tqdm\n",
    "from   os      import listdir\n",
    "from   os.path import isdir, isfile, join\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "#\n",
    "# Visualization Libraries\n",
    "#\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "\n",
    "# User libraries\n",
    "#\n",
    "from utils.utils         import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 192000\n",
    "audio_data  = librosa.load('./Data/S1Ep3_Sound/Dev_2/IMP23ABSU_MIC.wav', sr = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 7))\n",
    "\n",
    "librosa.display.waveplot( audio_data[0],  audio_data[1],  alpha=0.7 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram\n",
    " \n",
    "A spectrogram is a visual way of representing the signal strength, or “loudness”, of a signal over time at various frequencies present in a particular waveform. Not only can one see whether there is more or less energy at, for example, 2 Hz vs 10 Hz, but one can also see how energy levels vary over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X   = librosa.stft( audio_data[0] )\n",
    "Xdb = librosa.amplitude_to_db(abs(X))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(25, 7))\n",
    "librosa.display.specshow(Xdb, sr=audio_data[1], x_axis='time', y_axis='hz')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel-Frequency Cepstral Coefficients(MFCCs)\n",
    "\n",
    "The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10–20) which concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(audio_data[0], sr=audio_data[1])\n",
    "\n",
    "#Displaying  the MFCCs:\n",
    "#\n",
    "plt.figure(figsize=(15, 7))\n",
    "librosa.display.specshow(mfccs, sr=audio_data[1], x_axis='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma feature\n",
    "\n",
    "A chroma feature or vector is typically a 12-element feature vector indicating how much energy of each pitch class, {C, C#, D, D#, E, …, B}, is present in the signal. In short, It provides a robust way to describe a similarity measure between music pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromagram = librosa.feature.chroma_stft( audio_data[0], sr=audio_data[1] )\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.specshow( chromagram, x_axis='time', y_axis='chroma', cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from Audio signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Centroid\n",
    "\n",
    "The spectral centroid indicates at which frequency the energy of a spectrum is centered upon or in other words It indicates where the ” center of mass” for a sound is located. This is like a weighted mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# Normalising the spectral centroid for visualisation\n",
    "#\n",
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spectral_centroids = librosa.feature.spectral_centroid(audio_data[0][:sr//100], sr=audio_data[1])[0]\n",
    "\n",
    "\n",
    "# Computing the time variable for visualization\n",
    "#\n",
    "plt.figure(figsize=(25, 7))\n",
    "frames = range(len(spectral_centroids))\n",
    "t      = librosa.frames_to_time(frames)\n",
    "\n",
    "\n",
    "#Plotting the Spectral Centroid along the waveform\n",
    "#\n",
    "librosa.display.waveplot(audio_data[0], sr=audio_data[1], alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_centroids), color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_centroids.shape  # OLA -> 44825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Rolloff\n",
    "\n",
    "It is a measure of the shape of the signal. It represents the frequency at which high frequencies decline to 0. To obtain it, we have to calculate the fraction of bins in the power spectrum where 85% of its power is at lower frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_rolloff = librosa.feature.spectral_rolloff(audio_data[0]+0.01, sr=audio_data[1])[0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(25, 7))\n",
    "librosa.display.waveplot(audio_data[0], sr=audio_data[1], alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_rolloff), color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Bandwidth\n",
    "\n",
    "The spectral bandwidth is defined as the width of the band of light at one-half the peak maximum (or full width at half maximum [FWHM]) and is represented by the two vertical red lines and λSB on the wavelength axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_bandwidth_2 = librosa.feature.spectral_bandwidth(audio_data[0]+0.01, sr=audio_data[1], p=2)[0] # default\n",
    "spectral_bandwidth_3 = librosa.feature.spectral_bandwidth(audio_data[0]+0.01, sr=audio_data[1], p=3)[0]\n",
    "spectral_bandwidth_4 = librosa.feature.spectral_bandwidth(audio_data[0]+0.01, sr=audio_data[1], p=4)[0]\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(25, 7))\n",
    "#\n",
    "librosa.display.waveplot(audio_data[0], sr=audio_data[1], alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_bandwidth_2), color='r')\n",
    "plt.plot(t, normalize(spectral_bandwidth_3), color='g')\n",
    "plt.plot(t, normalize(spectral_bandwidth_4), color='y')\n",
    "plt.legend(('p = 2', 'p = 3', 'p = 4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Crossing Rate\n",
    "\n",
    "A very simple way for measuring the smoothness of a signal is to calculate the number of zero-crossing within a segment of that signal. A voice signal oscillates slowly — for example, a 100 Hz signal will cross zero 100 per second — whereas an unvoiced fricative can have 3000 zero crossings per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_crossings = librosa.zero_crossings(audio_data[0][:1000], pad=False)\n",
    "print(sum(zero_crossings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path\n",
    "#\n",
    "path              = './Data/S1Ep3_Sound/'\n",
    "SoundData         = {}\n",
    "\n",
    "\n",
    "for i in range(2, 6 ):\n",
    "    \n",
    "    legend = 'Dev_{}'.format(i)\n",
    "    print('[INFO] Case: ', legend)\n",
    "    \n",
    "\n",
    "    # Load Accelerometer data\n",
    "    #\n",
    "    df = pd.read_csv( path + 'Dev_{}/IMP23ABSU_MIC.csv'.format(i))\n",
    "    print('[INFO] Sound data loaded')\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    # Import specifications\n",
    "    #\n",
    "    AcquisitionInfo = pd.read_json(path + 'Dev_{}/AcquisitionInfo.json'.format(i))\n",
    "    DeviceConfig    = pd.read_json(path + 'Dev_{}/DeviceConfig.json'.format(i))\n",
    "    \n",
    "    print('[INFO] Name:   ', DeviceConfig['device']['deviceInfo']['alias'])\n",
    "    print('[INFO] Serial: ', DeviceConfig['device']['deviceInfo']['serialNumber'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    if ( (df['Time'].diff()[1:] <= 0.0).any() ):\n",
    "        print('[ERROR] Data are probably not correct')\n",
    "    else:\n",
    "        # Fix date-time and set index \n",
    "        #\n",
    "        df['Time_index']            = pd.to_timedelta( df['Time'], 's')\n",
    "        \n",
    "        # Set index\n",
    "        #\n",
    "        df.set_index('Time_index', inplace=True)    \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#     # Plot measurements\n",
    "#     #\n",
    "#     plt.figure( figsize=(25, 5) )    \n",
    "#     df[ 'MIC' ].plot( alpha=0.7 )        \n",
    "    \n",
    "#     plt.title( legend )\n",
    "#     plt.show()\n",
    "\n",
    "    break\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Store data\n",
    "    #\n",
    "    SoundData[ legend ]     = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signal = df['MIC'].to_numpy()\n",
    "\n",
    "Sample_rate = 52000\n",
    "\n",
    "Frequencies = np.linspace (0.0, Sample_rate//2, Signal.shape[0]//2)\n",
    "freq_data   = fft( Signal )\n",
    "Amplitudes  = np.abs (freq_data [0:Signal.shape[0]//2])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure( figsize=(25,7) )\n",
    "plt.plot(Frequencies, Amplitudes)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dev_ili_v3]",
   "language": "python",
   "name": "conda-env-.conda-dev_ili_v3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
